<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research">
  <meta name="keywords" content="AI Co-Scientist, SPOT, AI4Science">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Researchs</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
<!--   <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/apple-touch-icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/chartjs-plugin-datalabels@2"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://scholar.google.com/citations?user=Zf_eLDsAAAAJ&hl=en">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="">
            Upcoming!
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Researchs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Zf_eLDsAAAAJ&hl=en">Guijin Son</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://jiwooya1000.github.io/">Jiwoo Hong</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="https://honglu.fan/whoami/">Honglu Fan</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://hazel-heejeong-nam.github.io/">Heejeong Nam</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=ISuvyVYAAAAJ&hl=ko">Hyunwoo Ko</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://sngwonlim.github.io/">Seungwon Lim</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.kr/citations?user=CIfZvqcAAAAJ&hl=ko">Jinyeop Song</a><sup>6</sup>
            </span>
            <span class="author-block">
              <a href="">Jinha Choi</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.it/citations?hl=pt-PT&user=kiVYjZ4AAAAJ&view_op=list_works&sortby=pubdate">Gon√ßalo Paulo</a><sup>2</sup>
            </span>
            <span class="author-block">
              <a href="https://yj-yu.github.io/home/">Youngjae Yu</a><sup>5</sup>
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=bO7H0DAAAAAJ&hl=en">Stella Biderman</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>OneLineAI,</span>
            <span class="author-block"><sup>2</sup>EleutherAI,</span>
            <span class="author-block"><sup>3</sup>KAIST AI,</span>
            <span class="author-block"><sup>4</sup>Boeing Korea,</span>
            <span class="author-block"><sup>5</sup>Yonsei University,</span>
            <span class="author-block"><sup>6</sup>MIT</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2505.11855"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.11855"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/guijinSON/SPOT"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/amphora/SPOT-MetaData"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


  <!-- 1) The big, fun button -->
<div class="has-text-centered mb-5">
  <button
    id="show-leaderboard-btn"
    class="button is-primary is-large">
    üöÄ See Top Performing Models üåü
  </button>
</div>

<!-- 2) Hide this section until the button is clicked -->
<section
  class="section is-hidden"
  id="leaderboard">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-left">üåü Top Performing Models on SPOT üåü</h2>
    <div
      class="box"
      style="border:1px solid #ddd; padding:1rem; max-width:1000px; margin:0 auto;">
      <div class="columns">
        <!-- Recall -->
        <div class="column">
          <h3 class="subtitle is-5 has-text-centered">Recall‚Äâ%</h3>
          <div style="height:250px;">
            <canvas id="chart-recall"></canvas>
          </div>
        </div>
        <!-- Precision -->
        <div class="column">
          <h3 class="subtitle is-5 has-text-centered">Precision‚Äâ%</h3>
          <div style="height:250px;">
            <canvas id="chart-precision"></canvas>
          </div>
        </div>
        <!-- Pass@4 -->
        <div class="column">
          <h3 class="subtitle is-5 has-text-centered">Pass@4‚Äâ%</h3>
          <div style="height:250px;">
            <canvas id="chart-pass4"></canvas>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<script>
  document
    .getElementById('show-leaderboard-btn')
    .addEventListener('click', function() {
      // reveal the section
      const lb = document.getElementById('leaderboard');
      lb.classList.remove('is-hidden');
      // scroll it into view smoothly
      lb.scrollIntoView({ behavior: 'smooth' });
      // optionally hide the button itself
      this.style.display = 'none';
    });
</script>
  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work
casts these systems as generative co-authors responsible for crafting hypotheses,
synthesizing code, or drafting manuscripts. 
          </p>
          <p>
            In this work, we explore a complementary application: using LLMs as verifiers to automate the academic verification of scientific manuscripts. To that end, we introduce SPOT, a dataset of
83 published papers paired with 91 errors significant enough to prompt errata or
retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1% recall or
6.1% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs,
models rarely rediscover the same errors, undermining their reliability. Finally,
qualitative analysis with domain experts reveals that even the strongest models
make mistakes resembling student-level misconceptions derived from misunderstandings. 
          </p>
          <p>
            These findings highlight the substantial gap between current LLM
capabilities and the requirements for dependable AI-assisted academic verification.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>


<section id="data-curation" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 has-text-left">üîç Creating SPOT</h2>
    <div class="content">
      <figure class="image is-3by1">
        <img src="static/images/spot.png"
             alt="Diagram of the SPOT data curation pipeline stages">
        <figcaption class="has-text-centered is-size-7 mt-2">
          Figure: Overview of the five‚Äìstage SPOT data curation process.
        </figcaption>
      </figure>
      
      <p>
        We begin by <strong>Stage 1 ‚Äì Seed Collection</strong>, harvesting manuscripts flagged for critical errors from two primary sources: <em>WithdrarXiv</em> (self-retractions and errata comments) and PubPeer (post-publication peer reviews).  We extract entries annotated as factual, methodological, or other critical mistakes, scrape each paper‚Äôs metadata and full comments, then prune low-yield repositories (medRxiv/bioRxiv) to focus on high-signal samples.
      </p>
      <p>
        Next, in <strong>Stage 2</strong>, we run two GPT-4o filtering passes to isolate comments that point to a specific section, figure, equation, or table and drop those requiring external artifacts.  We then move to <strong>Stage 3</strong> author validation‚Äîonly keeping errors explicitly confirmed by the original authors‚Äîand <strong>Stage 4</strong> human sanity checks, where annotators verify self-containment, identifiability, and author acknowledgement.  Finally, in <strong>Stage 5 ‚Äì Normalization</strong>, we convert each PDF into text and images via Llama-Parse + GPT-4.1 refinement, ensuring high-fidelity OCR and visual captures of every figure, table, and equation.
      </p>

      
    </div>
  </div>
</section>

<section id="evaluation" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìä Evaluation Metrics & Main Results</h2>

    <!-- 1) Quick intro to metrics -->
    <p>
      We evaluate verification performance via <strong>Precision</strong>, <strong>Recall</strong>, and <code>pass@K</code>.  
      A predicted error is a <em>true positive</em> only when the model‚Äôs reported location matches a benchmark annotation (confirmed via GPT-4.1 similarity checks); all other flags count as false positives, and any missed annotations as false negatives:
    </p>
    <p>
      <strong>Precision</strong> = TP / (TP + FP) penalizes spurious flags,  
      while <strong>Recall</strong> = TP / (TP + FN) penalizes missed detections.  
      To measure multi‚Äêattempt gains, we report <code>pass@1</code> and <code>pass@4</code> over eight independent runs.
    </p>

    <!-- 2) Main results table -->
    <figure class="image" style="max-width:900px; margin:1.5rem auto;">
      <img src="static/images/table.png"
           alt="Precision, Recall, pass@1, pass@4 for ten multi-modal LLMs on SPOT"
           style="width:100%; height:auto;">
    </figure>

    <!-- 3) Highlight key takeaways -->
    <p>
      Even the strongest model, o3, achieves only 21.1 % recall and 6.1 % precision on SPOT, underscoring the challenge of automatically pinpointing real errors in full‚Äêlength scientific manuscripts. Open‚Äêsource counterparts like Qwen-2.5-VL-72B and Llama-4-Maverick collapse to near-zero performance.  This shows that while neither proprietary nor open-source models fully satisfy the requirements of practical deployments of error-detecting AI systems, open-source models lag far behind in domain-specific rigor and robust error-detection capabilities essential for scientific applications.
    </p>
<br>
     <p>
      Notably, reasoning models show strong performance in the Equation/Proof category, o3 leads with a 62.6 % pass@4 rate, followed by Gemini-2.5-Pro at 36.4 %, with all others below 5 %. However, they fall short on visual tasks: GPT-4.1 tops Figure Duplication at 44.4 %, while o3 and Gemini-2.5-Pro score 0 %. This contrast reveals that, despite impressive reasoning capablities on text, current reasoning LLMs show poor performance in grounding multimodal figure analysis.
    </p>
  </div>
</section>

  <section id="miscalibration" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-4 has-text-left">‚ö†Ô∏è Unreliability of Miscalibrated Models</h2>
    <div class="content">
      <p>
        Alongside <code>pass@4</code>, <strong>calibration</strong> tells us how much to trust a model‚Äôs own confidence estimates.  
        In scientific error detection, where chasing false positives can cost serious time and effort, knowing when to believe a model is crucial.  
      </p>
      <figure class="image" style="max-width: 800px; margin: 0 auto;">
        <img src="static/images/confidence.png" 
          style="width: 100%; height: auto;"
             alt="Density and scatter plots showing model confidence vs. pass@4 for six LLMs on SPOT">
      </figure>
      <p>
        However, as shown in Figure 4 (right), confidence correlates only weakly with <code>pass@4</code>, and the density plots on the left reveal that all models report very low confidence, clustering near zero.  
        Across 498 model‚Äìinstance evaluations (83 instances √ó 6 models), we observe just two cases of full (100%) confidence (both from o3), underscoring that today‚Äôs LLMs remain largely unreliable for spotting real errors in scientific manuscripts.
      </p>
    </div>
  </div>
</section>

  <section id="case-studies" class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">üìö Detailed Case Studies</h2>

    <!-- Mathematics Case Study -->
    <div class="box">
      <h3 class="title is-4">Mathematics: Petersen and Tommas (2024)</h3>

      <!-- Model Generations -->
      <div class="box">
        <h4 class="subtitle is-5">o3‚Äôs Feedback</h4>
        <div class="notification is-primary">
          Eichler‚ÄìShimura is quoted as <code>H¬π(M‚ÇÅ,‚ÇÅ,V‚Çó) ‚âÖ S_{‚Ñì+2} ‚äï ‚Ñö(‚àí1‚àí‚Ñì)</code>,  
          but the Tate (Eisenstein) summand should be <code>‚Ñö(‚àí1)</code>, independent of ‚Ñì.  
          The extra ‚Äú<code>‚àí‚Ñì</code>‚Äù is a mis-twist.
        </div>
      </div>
      <div class="box">
        <h4 class="subtitle is-5">Gemini-2.5-Pro‚Äôs Feedback</h4>
        <div class="notification is-info">
          The stated result <code>H¬π(M‚ÇÇ, V(2,2)) = ‚Ñö(‚àí3)</code> contradicts the Gysin‚Äêsequence derivation.  
          One finds instead <code>H¬π(M‚ÇÇ, V(2,2)) = 0</code> after tracking  
          <code>H‚Å∞(A‚ÇÅ,‚ÇÅ, V(2,2))(‚àí1) ‚Üí H¬≤(A‚ÇÇ)</code>, implying no non-zero cohomology.
        </div>
      </div>

      <!-- Analysis Toggle -->
      <div class="has-text-centered">
        <button class="button is-link is-light" data-target="analysis-math">
          üîç Show Expert Analysis
        </button>
      </div>
      <div id="analysis-math" class="content is-hidden mt-3">
        <p>
          Both o3 and Gemini-2.5-Pro exclusively flag issues in Section 3. 
          Ironically, this is the only part of the manuscript <strong>not</strong> affected by the actual mathematical gap. 
          o3 criticizes the calculation of <code>H^k(M_{1,1}, V_\ell)</code> in Section 3.3, claiming that the use of Eichler-Shimura isomorphism is wrong without giving too much details. 
          But there is no mistake in this part and the claim is hallucinated. 
          It is worth noting that applying Eichler-Shimura isomorphism in a similar context as the paper is exceedingly rare in internet data. 
          The formula may also appear differently than how the Eichler-Shimura isomorphism is presented in common internet sources, whose equivalence requires genuine understanding of multiple subjects, such as variation of Hodge structures and the theory of modular forms.
        </p>

        <p>
          On the other hand, Gemini-2.5-Pro tackles Theorem 3.8. 
          Despite doing reasonable calculations, the model makes a false claim on <code>H^0(A_{1,1}, V(2,2))(-1)</code>code>, resembling mistakes students could make after recollecting a wrong fact.
          </p> 
      </div>
    </div>

    <!-- Materials Science Case Study -->
    <div class="box">
      <h3 class="title is-4">Materials Science: Ye et al. (2025)</h3>

      <!-- Model Generations -->
      <div class="box">
        <h4 class="subtitle is-5">o3‚Äôs Feedback</h4>
        <div class="notification is-primary">
          Optimal TfOH concentration is stated as 0.006‚Äì0.016 M, yet all syntheses use 0.2 M.  
          This inconsistency invalidates the universality of the claimed optimum.
        </div>
      </div>
      <div class="box">
        <h4 class="subtitle is-5">Gemini-2.5-Pro‚Äôs Feedback</h4>
        <div class="notification is-info">
          <strong>(A)</strong> Indexed PXRD peaks incorrectly attribute facets to TAPPy-TFPPy-COF instead of TAPPy-BPTC-COF.<br><br>
          <strong>(B)</strong> The BET surface area is listed as <code>1606 cm¬≤ g‚Åª¬π</code> rather than  
          <code>1606 m¬≤ g‚Åª¬π</code>, a 10,000√ó unit error.
        </div>
      </div>

      <!-- Analysis Toggle -->
      <div class="has-text-centered">
        <button class="button is-link is-light" data-target="analysis-mat">
          üîç Show Expert Analysis
        </button>
      </div>
      <div id="analysis-mat" class="content is-hidden mt-3">
        <p>
          Errors pointed out by o3 exemplify failures in both long-range context comprehension and multi-hop reasoning.
          o3 does not realize that the optimal concentration value reported by the authors (0.006‚Äì0.016 M) is the concentration of the final mixture, while the the second value (0.2M) is the concentration of the acid before being added to the final mixture. 
          This misunderstanding likely arises because the optimal concentration in the final mixture is mentioned only once, and the explicit calculation is not shown throughout the manuscript. 
          As a result, o3, having seen references only to the concentration before mixture, fails to infer the relationship between the two values.
        </p>

         <p>
            In (A), Gemini 2.5 Pro seems to make a "reading" mistake, attributing the second facet pair to TAPPy-TFPPy-COF when it in fact describes TAPPy-BPTC-COF. 
           Notably, however, in (B), it notices a potential error in the units, where a certain compound was assigned a surface area 10000x smaller than all the other compounds in the same family. 
           Because the authors do not mention this extreme property of this material, we suspect that this is a real typo.
           </p>
      </div>
    </div>
  </div>
</section>

<script>
  // Toggle show/hide for analysis blocks
  document.querySelectorAll('button[data-target]').forEach(btn => {
    btn.addEventListener('click', () => {
      const target = document.getElementById(btn.getAttribute('data-target'));
      target.classList.toggle('is-hidden');
      btn.textContent = target.classList.contains('is-hidden')
        ? btn.textContent.replace('Hide', 'Show')
        : btn.textContent.replace('Show', 'Hide');
    });
  });
</script>

  
  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{son2025ai,
  title={When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research},
  author={Son, Guijin and Hong, Jiwoo and Fan, Honglu and Nam, Heejeong and Ko, Hyunwoo and Lim, Seungwon and Song, Jinyeop and Choi, Jinha and Paulo, Gon{\c{c}}alo and Yu, Youngjae and others},
  journal={arXiv preprint arXiv:2505.11855},
  year={2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  <script>
document.addEventListener('DOMContentLoaded', () => {
  // 0) Grab the canvases up front
  const ctxRecall    = document.getElementById('chart-recall').getContext('2d');
  const ctxPrecision = document.getElementById('chart-precision').getContext('2d');
  const ctxPass4     = document.getElementById('chart-pass4').getContext('2d');

  // 1) register the datalabels plugin
  Chart.register(ChartDataLabels);

  // 2) metric ‚Üí canvas context + colors
  const configs = {
    recall:    { ctx: ctxRecall,    bg:'rgba(54,162,235,0.5)',  border:'rgba(54,162,235,1)'  },
    precision: { ctx: ctxPrecision, bg:'rgba(75,192,192,0.5)', border:'rgba(75,192,192,1)' },
    'pass@4':  { ctx: ctxPass4,     bg:'rgba(255,205,86,0.5)', border:'rgba(255,205,86,1)' }
  };

  // 3) load the data
  fetch('static/data/leaderboard.json')
    .then(res => res.json())
    .then(({ models }) => {
      // once loaded, draw all charts
      Object.entries(configs).forEach(([metric, {ctx,bg,border}]) => {
        // sort descending
        const sorted = [...models].sort((a,b)=>b[metric]-a[metric]);
        const fullNames  = sorted.map(m=>m.name);
        const labels     = fullNames.map(n=>n.split(' (')[0]);
        const values     = sorted.map(m=>m[metric]);
        const maxVal     = Math.max(...values);

        new Chart(ctx, {
          type: 'bar',
          data: {
            labels,
            datasets: [{
              data: values,
              backgroundColor: bg,
              borderColor:     border,
              borderWidth: 1
            }]
          },
          options: {
            indexAxis: 'y',
            maintainAspectRatio: false,
            scales: {
              x: { beginAtZero:true, max: maxVal + 10 },
              y: { ticks:{ font:{size:11} } }
            },
            plugins: {
              legend:{display:false},
              tooltip:{
                callbacks:{
                  title: items => [ fullNames[items[0].dataIndex] ]
                }
              },
              datalabels:{
                anchor:'end',
                align:'end',
                formatter:v=>v.toFixed(1)+'%',
                font:{ weight:'bold' }
              }
            }
          }
        });
      });
    })
    .catch(err => console.error('Leaderboard JSON load failed:', err));

  // 4) your button logic to un-hide it
  document
    .getElementById('show-leaderboard-btn')
    .addEventListener('click', function() {
      const lb = document.getElementById('leaderboard');
      lb.classList.remove('is-hidden');
      lb.scrollIntoView({behavior:'smooth'});
      this.style.display = 'none';
    });
});
</script>

  
</body>
</html>
